{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ_RYIc8Toed"
      },
      "outputs": [],
      "source": [
        "url='https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import ast\n",
        "\n",
        "# Download NLTK data (run once)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "E6c8f799f3cu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2ac3fd-fcb6-45e8-acaa-9edca23af274"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_and_filter_data(filepath):\n",
        "    \"\"\"Load and filter the dataset to include only necessary and enhanced columns.\"\"\"\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(filepath)\n",
        "    # Select relevant columns\n",
        "    columns = ['title', 'overview', 'genres', 'tagline', 'popularity', 'vote_average', 'revenue']\n",
        "    df_filtered = df[columns]\n",
        "\n",
        "    # Drop rows with missing values in 'title' or 'overview'\n",
        "    df_filtered = df_filtered.dropna(subset=['title', 'overview'])\n",
        "\n",
        "    # Fill missing values in other columns\n",
        "    df_filtered['tagline'] = df_filtered['tagline'].fillna('')\n",
        "    df_filtered['genres'] = df_filtered['genres'].fillna('[]')\n",
        "    df_filtered['popularity'] = df_filtered['popularity'].fillna(0)\n",
        "    df_filtered['vote_average'] = df_filtered['vote_average'].fillna(0)\n",
        "    df_filtered['revenue'] = df_filtered['revenue'].fillna(0)\n",
        "\n",
        "    # Convert genres from string to list of dictionaries\n",
        "    df_filtered['genres'] = df_filtered['genres'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "\n",
        "    # Extract genre names\n",
        "    df_filtered['genres'] = df_filtered['genres'].apply(lambda x: ', '.join([g['name'] for g in x]))\n",
        "\n",
        "    # Combine 'overview', 'tagline', and 'genres' into a single text feature\n",
        "    df_filtered['combined_text'] = df_filtered['overview'] + ' ' + df_filtered['tagline'] + ' ' + df_filtered['genres']\n",
        "\n",
        "    # Preprocess the combined text\n",
        "    df_filtered['combined_text'] = df_filtered['combined_text'].apply(preprocess_text)\n",
        "\n",
        "    return df_filtered\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text by lowercasing, removing special characters, and lemmatizing.\"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "def vectorize_text(df):\n",
        "    \"\"\"Convert combined text into TF-IDF vectors.\"\"\"\n",
        "    tfidf = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1, 2))\n",
        "    tfidf_matrix = tfidf.fit_transform(df['combined_text'])\n",
        "    return tfidf, tfidf_matrix\n",
        "\n",
        "def recommend_movies(query, df, tfidf, tfidf_matrix, top_n=5):\n",
        "    \"\"\"Recommend movies based on user query.\"\"\"\n",
        "    # Preprocess query\n",
        "    query = preprocess_text(query)\n",
        "    # Transform query into TF-IDF vector\n",
        "    query_vector = tfidf.transform([query])\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "\n",
        "    # Get top N recommendations\n",
        "    top_indices = similarity_scores.argsort()[-top_n:][::-1]\n",
        "    recommendations = df.iloc[top_indices].copy()\n",
        "    recommendations.loc[:, 'similarity'] = similarity_scores[top_indices]  # Fix SettingWithCopyWarning\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and filter data\n",
        "    df = load_and_filter_data('/content/drive/MyDrive/SEM-3/movies_metadata.csv')\n",
        "\n",
        "\n",
        "    # Vectorize text\n",
        "    tfidf, tfidf_matrix = vectorize_text(df)\n",
        "\n",
        "    # Ask the user for input\n",
        "    query = input(\"What kind of movies do you like? (e.g., 'I love thrilling action movies set in space'): \")\n",
        "\n",
        "    # Get recommendations\n",
        "    recommendations = recommend_movies(query, df, tfidf, tfidf_matrix)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nTop 5 Recommended Movies:\")\n",
        "    for i, row in recommendations.iterrows():\n",
        "        print(f\"{row['title']} (Similarity: {row['similarity']:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIqH6AK1Y0FC",
        "outputId": "36e09105-f2c9-40f8-d46d-815839878ecf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-c0230dd70810>:4: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(filepath)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What kind of movies do you like? (e.g., 'I love thrilling action movies set in space'): comedy\n",
            "\n",
            "Top 5 Recommended Movies:\n",
            "Money Is Not Everything (Similarity: 1.00)\n",
            "Afstiros katallilo (Similarity: 1.00)\n",
            "Job, czyli ostatnia szara kom√≥rka (Similarity: 1.00)\n",
            "Cabbages and Kings (Similarity: 1.00)\n",
            "Mr. Kuka's Advice (Similarity: 1.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ge7lMRmKp2c7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}